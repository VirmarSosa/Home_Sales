{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8213e4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "The system cannot find the path specified.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open '$SPARK_VERSION-bin-hadoop3.tgz'\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unable to find py4j in /content/spark-3.4.0-bin-hadoop3\\python, your SPARK_HOME may not be configured correctly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             \u001b[0mpy4j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lib\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"py4j-*.zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15768\\4126539101.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Start a SparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[0mpy4j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lib\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"py4j-*.zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             raise Exception(\n\u001b[0m\u001b[0;32m    162\u001b[0m                 \"Unable to find py4j in {}, your SPARK_HOME may not be configured correctly\".format(\n\u001b[0;32m    163\u001b[0m                     \u001b[0mspark_python\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Unable to find py4j in /content/spark-3.4.0-bin-hadoop3\\python, your SPARK_HOME may not be configured correctly"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Find the latest version of spark 3.x  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.3.1'\n",
    "spark_version = 'spark-3.4.0'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e32b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee9011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read in the AWS S3 bucket into a DataFrame.\n",
    "from pyspark import SparkFiles\n",
    "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"home_sales_revised.csv\"), header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4cd4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a temporary view of the DataFrame.\n",
    "\n",
    "df.createOrReplaceTempView('home_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b963ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. What is the average price for a four bedroom house sold in each year rounded to two decimal places?\n",
    "\n",
    "a = \"\"\"\n",
    "SELECT\n",
    "  YEAR(date) AS YEAR,\n",
    "  ROUND(AVG(price), 2) AS AVERAGE_PRICE\n",
    "FROM home_sales\n",
    "WHERE bedrooms = 4\n",
    "GROUP BY YEAR\n",
    "ORDER BY YEAR DESC\n",
    "\"\"\"\n",
    "spark.sql(a).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc48a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. What is the average price of a home for each year the home was built that have 3 bedrooms and 3 bathrooms rounded to two decimal places?\n",
    "b = \"\"\"\n",
    "SELECT\n",
    "  date_built AS `YEAR BUILT`,\n",
    "  ROUND(AVG(price), 2) AS AVERAGE_PRICE\n",
    "FROM home_sales\n",
    "WHERE bedrooms = 3 AND bathrooms = 3\n",
    "GROUP BY date_built\n",
    "ORDER BY date_built DESC\n",
    "\"\"\"\n",
    "spark.sql(b).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d884c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors,\n",
    "# and are greater than or equal to 2,000 square feet rounded to two decimal places?\n",
    "\n",
    "c = \"\"\"\n",
    "SELECT\n",
    "  date_built AS `YEAR BUILT`,\n",
    "  ROUND(AVG(price), 2) AS AVERAGE_PRICE\n",
    "FROM home_sales\n",
    "WHERE bedrooms = 3\n",
    "and bathrooms = 3\n",
    "and sqft_living >= 2000\n",
    "and floors = 2\n",
    "GROUP BY `YEAR BUILT`\n",
    "ORDER BY `YEAR BUILT` DESC\n",
    "\"\"\"\n",
    "spark.sql(c).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8d99fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. What is the \"view\" rating for the average price of a home, rounded to two decimal places, where the homes are greater than\n",
    "# or equal to $350,000? Although this is a small dataset, determine the run time for this query.\n",
    "\n",
    "start_time = time.time()\n",
    "d = \"\"\"\n",
    "SELECT\n",
    "  view, \n",
    "  ROUND(AVG(price), 2) AS AVERAGE_PRICE\n",
    "FROM home_sales\n",
    "GROUP BY view\n",
    "HAVING AVG(price) >= 350000\n",
    "ORDER BY view desc\n",
    "\"\"\"\n",
    "spark.sql(d).show()\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c73ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Cache the the temporary table home_sales.\n",
    "spark.sql('cache table home_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ad93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Check if the table is cached.\n",
    "spark.catalog.isCached('home_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ab06ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Using the cached data, run the query that filters out the view ratings with average price \n",
    "#  greater than or equal to $350,000. Determine the runtime and compare it to uncached runtime.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "e = \"\"\"\n",
    "SELECT\n",
    "  view,\n",
    "  ROUND(AVG(price), 2) AS AVERAGE_PRICE\n",
    "FROM home_sales\n",
    "GROUP BY view\n",
    "HAVING AVG(price) >= 350000\n",
    "ORDER BY view desc\n",
    "\"\"\"\n",
    "spark.sql(e).show()\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df230f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Partition by the \"date_built\" field on the formatted parquet home sales data \n",
    "df.write.partitionBy('date_built').parquet('p_home_sales',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db23e037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Read the parquet formatted data.\n",
    "p_homes_df = spark.read.parquet('p_home_sales')\n",
    "p_homes_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a386baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Create a temporary table for the parquet data.\n",
    "spark.sql('uncache table home_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca16c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Run the query that filters out the view ratings with average price of greater than or equal to $350,000 \n",
    "# with the parquet DataFrame. Round your average to two decimal places. \n",
    "# Determine the runtime and compare it to the cached version.\n",
    "\n",
    "start_time = time.time()\n",
    "e = \"\"\"\n",
    "SELECT\n",
    "  view,\n",
    "  ROUND(AVG(price), 2) AS AVERAGE_PRICE\n",
    "FROM home_sales\n",
    "GROUP BY view\n",
    "HAVING AVG(price) >= 350000\n",
    "ORDER BY view desc\n",
    "\"\"\"\n",
    "spark.sql(e).show()\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d96ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Uncache the home_sales temporary table.\n",
    "spark.sql('uncache table home_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1edf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Check if the home_sales is no longer cached\n",
    "if spark.catalog.isCached('home_sales'):\n",
    "  print('home_sales is cached')\n",
    "else:\n",
    "  print('not cached')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
